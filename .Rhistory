# Step 6: Model assessment
# Calculate predictions on the testing data
predictions_q1 <- predict(model_q1, testing_q1[, features_q1])
# Calculate evaluation metrics (e.g., RMSE)
accuracy_q1 <- sqrt(mean((predictions_q1 - testing_q1$AVGFPS)^2))
print("Evaluation of Model Performance")
print("RMSE (Accuracy):")
print(accuracy_q1)
# Additional evaluation metrics (e.g., R-squared)
rsquared_q1 <- 1 - (sum((testing_q1$AVGFPS - predictions_q1)^2) /
sum((testing_q1$AVGFPS - mean(testing_q1$AVGFPS))^2))
print("R-squared:")
print(rsquared_q1)
# Cross-validation (optional)
cv_q1 <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation
model_cv_q1 <- train(AVGFPS ~ ., data = data_q1[, c(features_q1, target_q1)],
method = "rpart", trControl = cv_q1)
print("Cross-validated RMSE:")
print(sqrt(model_cv_q1$results$RMSE))
# Step 7: Plot the decision tree
library(rpart.plot)
par(mfrow = c(1,1))
rpart.plot(model_q1, roundint = FALSE, fallen.leaves = TRUE,
main = "Decision Tree for AVGFPS Prediction")
# Step 8: Interpretation (optional)
print("Interpretation")
print("Decision Tree Model:")
print(model_q1)
print("Variable Importance Analysis:")
print(varImp(model_q1))
# Extract actual values from the testing data set
actual_q1 <- testing_q1$AVGFPS
# Create a data frame with both predicted and actual values
comparison_q1 <- data.frame(Predicted = predictions_q1, Actual = actual_q1)
# Step 9: Plot predicted vs. actual values
#par(mfrow = c(1,2))
#plot(comparison_q1$Actual, type = "l", col = "blue",
#     ylim = range(c(comparison_q1$Predicted, comparison_q1$Actual)),
#     xlab = "Observation", ylab = "PE", main = "Predicted vs. Actual AVGFPS")
#lines(comparison_q1$Predicted, col = "red")
#legend(x = 100, y = 35, legend = c("Actual", "Predicted"), col = c("blue", "red"), lty = 1)
# Step 10: Plot residuals
#residuals_q1 <- comparison_q1$Actual - comparison_q1$Predicted
#plot(residuals_q1, type = "p", col = "green",
#     xlab = "Observation", ylab = "Residuals", main = "Residuals Plot")
#abline(h = 0, col = "red")
#grid()
n
# Q2: Which of the technical variables (AA/AG/AP) have the most influence on PE?
# Step 0: Install necessary packages
# install.packages("caret")
# install.packages("rpart.plot")
# Step 1: Load and preprocess the data set
data_q1 <- read.csv("dataset_for_PE.csv")
# Step 2: Define feature and target variables
features_q1 <- c("AA", "AG", "AP")
target_q1 <- "AVGFPS"
# Step 3: Build decision tree model
library(rpart)
model_q1 <- rpart(AVGFPS ~ ., data = data_q1[, c(features_q1, target_q1)], method = "anova")
# Step 4: Split data set into training and testing sets
library(caret)
set.seed(321)
trainIndex_q1 <- createDataPartition(data_q1$AVGFPS, p = 0.7, list = FALSE)
training_q1 <- data_q1[trainIndex_q1, ]
testing_q1 <- data_q1[-trainIndex_q1, ]
# Step 5: Train the decision tree model
model_q1 <- rpart(AVGFPS ~ ., data = training_q1[, c(features_q1, target_q1)], method = "anova")
# Step 6: Model assessment
# Calculate predictions on the testing data
predictions_q1 <- predict(model_q1, testing_q1[, features_q1])
# Calculate evaluation metrics (e.g., RMSE)
accuracy_q1 <- sqrt(mean((predictions_q1 - testing_q1$AVGFPS)^2))
print("Evaluation of Model Performance")
print("RMSE (Accuracy):")
print(accuracy_q1)
# Additional evaluation metrics (e.g., R-squared)
rsquared_q1 <- 1 - (sum((testing_q1$AVGFPS - predictions_q1)^2) /
sum((testing_q1$AVGFPS - mean(testing_q1$AVGFPS))^2))
print("R-squared:")
print(rsquared_q1)
# Cross-validation (optional)
cv_q1 <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation
model_cv_q1 <- train(AVGFPS ~ ., data = data_q1[, c(features_q1, target_q1)],
method = "rpart", trControl = cv_q1)
print("Cross-validated RMSE:")
print(sqrt(model_cv_q1$results$RMSE))
# Step 7: Plot the decision tree
library(rpart.plot)
par(mfrow = c(1,1))
rpart.plot(model_q1, roundint = FALSE, fallen.leaves = TRUE,
main = "Decision Tree for AVGFPS Prediction")
# Step 8: Interpretation (optional)
print("Interpretation")
print("Decision Tree Model:")
print(model_q1)
print("Variable Importance Analysis:")
print(varImp(model_q1))
# Extract actual values from the testing data set
actual_q1 <- testing_q1$AVGFPS
# Create a data frame with both predicted and actual values
comparison_q1 <- data.frame(Predicted = predictions_q1, Actual = actual_q1)
# Step 9: Plot predicted vs. actual values
#par(mfrow = c(1,2))
#plot(comparison_q1$Actual, type = "l", col = "blue",
#     ylim = range(c(comparison_q1$Predicted, comparison_q1$Actual)),
#     xlab = "Observation", ylab = "PE", main = "Predicted vs. Actual AVGFPS")
#lines(comparison_q1$Predicted, col = "red")
#legend(x = 100, y = 35, legend = c("Actual", "Predicted"), col = c("blue", "red"), lty = 1)
# Step 10: Plot residuals
#residuals_q1 <- comparison_q1$Actual - comparison_q1$Predicted
#plot(residuals_q1, type = "p", col = "green",
#     xlab = "Observation", ylab = "Residuals", main = "Residuals Plot")
#abline(h = 0, col = "red")
#grid()
# the % in the nodes represent the percentage of observations (n=455)
View(testing_q1)
# Q2: Which of the technical variables (AA/AG/AP) have the most influence on PE?
# Step 0: Install necessary packages
# install.packages("caret")
# install.packages("rpart.plot")
# Step 1: Load and preprocess the data set
data_q1 <- read.csv("dataset_for_PE.csv")
# Step 2: Define feature and target variables
features_q1 <- c("AA", "AG", "AP")
target_q1 <- "AVGFPS"
# Step 3: Build decision tree model
library(rpart)
model_q1 <- rpart(AVGFPS ~ ., data = data_q1[, c(features_q1, target_q1)], method = "anova")
# Step 4: Split data set into training and testing sets
library(caret)
set.seed(321)
trainIndex_q1 <- createDataPartition(data_q1$PCP, p = 0.7, list = FALSE)
training_q1 <- data_q1[trainIndex_q1, ]
testing_q1 <- data_q1[-trainIndex_q1, ]
# Step 5: Train the decision tree model
model_q1 <- rpart(AVGFPS ~ ., data = training_q1[, c(features_q1, target_q1)], method = "anova")
# Step 6: Model assessment
# Calculate predictions on the testing data
predictions_q1 <- predict(model_q1, testing_q1[, features_q1])
# Calculate evaluation metrics (e.g., RMSE)
accuracy_q1 <- sqrt(mean((predictions_q1 - testing_q1$AVGFPS)^2))
print("Evaluation of Model Performance")
print("RMSE (Accuracy):")
print(accuracy_q1)
# Additional evaluation metrics (e.g., R-squared)
rsquared_q1 <- 1 - (sum((testing_q1$AVGFPS - predictions_q1)^2) /
sum((testing_q1$AVGFPS - mean(testing_q1$AVGFPS))^2))
print("R-squared:")
print(rsquared_q1)
# Cross-validation (optional)
cv_q1 <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation
model_cv_q1 <- train(AVGFPS ~ ., data = data_q1[, c(features_q1, target_q1)],
method = "rpart", trControl = cv_q1)
print("Cross-validated RMSE:")
print(sqrt(model_cv_q1$results$RMSE))
# Step 7: Plot the decision tree
library(rpart.plot)
par(mfrow = c(1,1))
rpart.plot(model_q1, roundint = FALSE, fallen.leaves = TRUE,
main = "Decision Tree for AVGFPS Prediction")
# Step 8: Interpretation (optional)
print("Interpretation")
print("Decision Tree Model:")
print(model_q1)
print("Variable Importance Analysis:")
print(varImp(model_q1))
# Extract actual values from the testing data set
actual_q1 <- testing_q1$AVGFPS
# Create a data frame with both predicted and actual values
comparison_q1 <- data.frame(Predicted = predictions_q1, Actual = actual_q1)
# Step 9: Plot predicted vs. actual values
#par(mfrow = c(1,2))
#plot(comparison_q1$Actual, type = "l", col = "blue",
#     ylim = range(c(comparison_q1$Predicted, comparison_q1$Actual)),
#     xlab = "Observation", ylab = "PE", main = "Predicted vs. Actual AVGFPS")
#lines(comparison_q1$Predicted, col = "red")
#legend(x = 100, y = 35, legend = c("Actual", "Predicted"), col = c("blue", "red"), lty = 1)
# Step 10: Plot residuals
#residuals_q1 <- comparison_q1$Actual - comparison_q1$Predicted
#plot(residuals_q1, type = "p", col = "green",
#     xlab = "Observation", ylab = "Residuals", main = "Residuals Plot")
#abline(h = 0, col = "red")
#grid()
# the % in the nodes represent the percentage of observations (n=455)
View(testing_q1)
# Q2: Which of the technical variables (AA/AG/AP) have the most influence on PE?
# Step 0: Install necessary packages
# install.packages("caret")
# install.packages("rpart.plot")
# Step 1: Load and preprocess the data set
data_q1 <- read.csv("dataset_for_PE.csv")
# Step 2: Define feature and target variables
features_q1 <- c("AA", "AG", "AP")
target_q1 <- "AVGFPS"
# Step 3: Build decision tree model
library(rpart)
model_q1 <- rpart(AVGFPS ~ ., data = data_q1[, c(features_q1, target_q1)], method = "anova")
# Step 4: Split data set into training and testing sets
library(caret)
set.seed(321)
trainIndex_q1 <- createDataPartition(data_q1$LEVEL, p = 0.7, list = FALSE)
training_q1 <- data_q1[trainIndex_q1, ]
testing_q1 <- data_q1[-trainIndex_q1, ]
# Step 5: Train the decision tree model
model_q1 <- rpart(AVGFPS ~ ., data = training_q1[, c(features_q1, target_q1)], method = "anova")
# Step 6: Model assessment
# Calculate predictions on the testing data
predictions_q1 <- predict(model_q1, testing_q1[, features_q1])
# Calculate evaluation metrics (e.g., RMSE)
accuracy_q1 <- sqrt(mean((predictions_q1 - testing_q1$AVGFPS)^2))
print("Evaluation of Model Performance")
print("RMSE (Accuracy):")
print(accuracy_q1)
# Additional evaluation metrics (e.g., R-squared)
rsquared_q1 <- 1 - (sum((testing_q1$AVGFPS - predictions_q1)^2) /
sum((testing_q1$AVGFPS - mean(testing_q1$AVGFPS))^2))
print("R-squared:")
print(rsquared_q1)
# Cross-validation (optional)
cv_q1 <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation
model_cv_q1 <- train(AVGFPS ~ ., data = data_q1[, c(features_q1, target_q1)],
method = "rpart", trControl = cv_q1)
print("Cross-validated RMSE:")
print(sqrt(model_cv_q1$results$RMSE))
# Step 7: Plot the decision tree
library(rpart.plot)
par(mfrow = c(1,1))
rpart.plot(model_q1, roundint = FALSE, fallen.leaves = TRUE,
main = "Decision Tree for AVGFPS Prediction")
# Step 8: Interpretation (optional)
print("Interpretation")
print("Decision Tree Model:")
print(model_q1)
print("Variable Importance Analysis:")
print(varImp(model_q1))
# Extract actual values from the testing data set
actual_q1 <- testing_q1$AVGFPS
# Create a data frame with both predicted and actual values
comparison_q1 <- data.frame(Predicted = predictions_q1, Actual = actual_q1)
# Step 9: Plot predicted vs. actual values
#par(mfrow = c(1,2))
#plot(comparison_q1$Actual, type = "l", col = "blue",
#     ylim = range(c(comparison_q1$Predicted, comparison_q1$Actual)),
#     xlab = "Observation", ylab = "PE", main = "Predicted vs. Actual AVGFPS")
#lines(comparison_q1$Predicted, col = "red")
#legend(x = 100, y = 35, legend = c("Actual", "Predicted"), col = c("blue", "red"), lty = 1)
# Step 10: Plot residuals
#residuals_q1 <- comparison_q1$Actual - comparison_q1$Predicted
#plot(residuals_q1, type = "p", col = "green",
#     xlab = "Observation", ylab = "Residuals", main = "Residuals Plot")
#abline(h = 0, col = "red")
#grid()
# the % in the nodes represent the percentage of observations (n=455)
# Q2: Which of the technical variables (AA/AG/AP) have the most influence on PE?
# Step 0: Install necessary packages
# install.packages("caret")
# install.packages("rpart.plot")
# Step 1: Load and preprocess the data set
data_q1 <- read.csv("dataset_for_PE.csv")
# Step 2: Define feature and target variables
features_q1 <- c("AA", "AG", "AP")
target_q1 <- "AVGFPS"
# Step 3: Build decision tree model
library(rpart)
model_q1 <- rpart(AVGFPS ~ ., data = data_q1[, c(features_q1, target_q1)], method = "anova")
# Step 4: Split data set into training and testing sets
library(caret)
set.seed(321)
trainIndex_q1 <- createDataPartition(data_q1$AVGFPS, p = 0.7, list = FALSE)
training_q1 <- data_q1[trainIndex_q1, ]
testing_q1 <- data_q1[-trainIndex_q1, ]
# Step 5: Train the decision tree model
model_q1 <- rpart(AVGFPS ~ ., data = training_q1[, c(features_q1, target_q1)], method = "anova")
# Step 6: Model assessment
# Calculate predictions on the testing data
predictions_q1 <- predict(model_q1, testing_q1[, features_q1])
# Calculate evaluation metrics (e.g., RMSE)
accuracy_q1 <- sqrt(mean((predictions_q1 - testing_q1$AVGFPS)^2))
print("Evaluation of Model Performance")
print("RMSE (Accuracy):")
print(accuracy_q1)
# Additional evaluation metrics (e.g., R-squared)
rsquared_q1 <- 1 - (sum((testing_q1$AVGFPS - predictions_q1)^2) /
sum((testing_q1$AVGFPS - mean(testing_q1$AVGFPS))^2))
print("R-squared:")
print(rsquared_q1)
# Cross-validation (optional)
cv_q1 <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation
model_cv_q1 <- train(AVGFPS ~ ., data = data_q1[, c(features_q1, target_q1)],
method = "rpart", trControl = cv_q1)
print("Cross-validated RMSE:")
print(sqrt(model_cv_q1$results$RMSE))
# Step 7: Plot the decision tree
library(rpart.plot)
par(mfrow = c(1,1))
rpart.plot(model_q1, roundint = FALSE, fallen.leaves = TRUE,
main = "Decision Tree for AVGFPS Prediction")
# Step 8: Interpretation (optional)
print("Interpretation")
print("Decision Tree Model:")
print(model_q1)
print("Variable Importance Analysis:")
print(varImp(model_q1))
# Extract actual values from the testing data set
actual_q1 <- testing_q1$AVGFPS
# Create a data frame with both predicted and actual values
comparison_q1 <- data.frame(Predicted = predictions_q1, Actual = actual_q1)
# Step 9: Plot predicted vs. actual values
#par(mfrow = c(1,2))
#plot(comparison_q1$Actual, type = "l", col = "blue",
#     ylim = range(c(comparison_q1$Predicted, comparison_q1$Actual)),
#     xlab = "Observation", ylab = "PE", main = "Predicted vs. Actual AVGFPS")
#lines(comparison_q1$Predicted, col = "red")
#legend(x = 100, y = 35, legend = c("Actual", "Predicted"), col = c("blue", "red"), lty = 1)
# Step 10: Plot residuals
#residuals_q1 <- comparison_q1$Actual - comparison_q1$Predicted
#plot(residuals_q1, type = "p", col = "green",
#     xlab = "Observation", ylab = "Residuals", main = "Residuals Plot")
#abline(h = 0, col = "red")
#grid()
# the % in the nodes represent the percentage of observations (n=455)
View(data_q1)
data <- read.csv("dataset_for_q2.csv")
# Summary statistics for numeric variables
summary(data[, c("Levels", "Score", "AverageFPS", "GamingInterest", "ParticipantNumber")])
# Summary statistics for numeric variables
summary(data[, c("LEVEL", "SCORE", "AVGFPS", "GAMING", "PCP")])
# Frequency table for categorical variables
table(data$AGG)
table(data$SEX)
View(data)
# Frequency table for categorical variables
table(data$AGE)
# Histograms for numeric variables
par(mfrow = c(3,2))
hist(data$LEVEL, main = "Distribution of Levels")
hist(data$SCORE, main = "Distribution of Score")
hist(data$AVGFPS, main = "Distribution of Average FPS")
hist(data$GAMING, main = "Distribution of Gaming Interest")
hist(data$PCP, main = "Distribution of Participant Number")
# Bar plots for categorical variables
barplot(table(data$AGE), main = "Frequency of Age Groups")
# Histograms for numeric variables
par(mfrow = c(2,3))
hist(data$LEVEL, main = "Distribution of Levels")
hist(data$SCORE, main = "Distribution of Score")
hist(data$AVGFPS, main = "Distribution of Average FPS")
hist(data$GAMING, main = "Distribution of Gaming Interest")
hist(data$PCP, main = "Distribution of Participant Number")
# Bar plots for categorical variables
barplot(table(data$AGE), main = "Frequency of Age Groups")
barplot(table(data$SEX), main = "Frequency of Sex")
# Bar plots for categorical variables
barplot(table(data$AGE), main = "Frequency of Age Groups")
barplot(table(data$SEX), main = "Frequency of Sex")
# Bar plots for categorical variables
par(mfrow = c(1,2))
barplot(table(data$AGE), main = "Frequency of Age Groups")
barplot(table(data$SEX), main = "Frequency of Sex")
# Create a new variable to indicate the level within each participant's data
data$LevelWithinParticipant <- rep(1:27, each = 24)
data <- read.csv("dataset_for_q2.csv")
# Create a new variable to indicate the level within each participant's data
data$LevelWithinParticipant <- rep(1:27, each = 24)
View(data)
# Now, you can use this new variable to subset your data
# For example, to extract the AGE and SEX data for each participant:
participant_data <- aggregate(cbind(AGE, SEX) ~ LevelWithinParticipant, data = data, FUN = function(x) x[1])
View(participant_data)
# Now, you can use this new variable to subset your data
# For example, to extract the AGE and SEX data for each participant:
participant_data <- aggregate(cbind(AGE, SEX) ~ LevelWithinParticipant, data = data, FUN = function(x) x[2])
# Now, you can use this new variable to subset your data
# For example, to extract the AGE and SEX data for each participant:
participant_data <- aggregate(cbind(AGE, SEX) ~ LevelWithinParticipant, data = data, FUN = function(x) x[3])
View(data)
# Q1: Which of the technical variables (AA/AG/AP) have the most influence on AVGFPS?
# Step 0: Install necessary packages
# install.packages("caret")
# install.packages("rpart.plot")
# Step 1: Load and preprocess the data set
data_q1 <- read.csv("dataset_for_PE.csv")
# Step 2: Define feature and target variables
features_q1 <- c("AA", "AG", "AP")
target_q1 <- "AVGFPS"
# Step 3: Build decision tree model
library(rpart)
model_q1 <- rpart(AVGFPS ~ ., data = data_q1[, c(features_q1, target_q1)], method = "anova")
# Step 4: Split data set into training and testing sets
library(caret)
set.seed(321)
trainIndex_q1 <- createDataPartition(data_q1$AVGFPS, p = 0.7, list = FALSE)
training_q1 <- data_q1[trainIndex_q1, ]
testing_q1 <- data_q1[-trainIndex_q1, ]
# Step 5: Train the decision tree model
model_q1 <- rpart(AVGFPS ~ ., data = training_q1[, c(features_q1, target_q1)], method = "anova")
# Step 6: Model assessment
# Calculate predictions on the testing data
predictions_q1 <- predict(model_q1, testing_q1[, features_q1])
# Calculate evaluation metrics (e.g., RMSE)
accuracy_q1 <- sqrt(mean((predictions_q1 - testing_q1$AVGFPS)^2))
print("Evaluation of Model Performance")
print("RMSE (Accuracy):")
print(accuracy_q1)
# Additional evaluation metrics (e.g., R-squared)
rsquared_q1 <- 1 - (sum((testing_q1$AVGFPS - predictions_q1)^2) /
sum((testing_q1$AVGFPS - mean(testing_q1$AVGFPS))^2))
print("R-squared:")
print(rsquared_q1)
# Cross-validation (optional)
cv_q1 <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation
model_cv_q1 <- train(AVGFPS ~ ., data = data_q1[, c(features_q1, target_q1)],
method = "rpart", trControl = cv_q1)
print("Cross-validated RMSE:")
print(sqrt(model_cv_q1$results$RMSE))
# Step 7: Plot the decision tree
library(rpart.plot)
par(mfrow = c(1,1))
rpart.plot(model_q1, roundint = FALSE, fallen.leaves = TRUE,
main = "Decision Tree for AVGFPS Prediction")
# Step 8: Interpretation (optional)
print("Interpretation")
print("Decision Tree Model:")
print(model_q1)
print("Variable Importance Analysis:")
print(varImp(model_q1))
# Extract actual values from the testing data set
actual_q1 <- testing_q1$AVGFPS
# Create a data frame with both predicted and actual values
comparison_q1 <- data.frame(Predicted = predictions_q1, Actual = actual_q1)
# Step 9: Plot predicted vs. actual values
#par(mfrow = c(1,2))
#plot(comparison_q1$Actual, type = "l", col = "blue",
#     ylim = range(c(comparison_q1$Predicted, comparison_q1$Actual)),
#     xlab = "Observation", ylab = "PE", main = "Predicted vs. Actual AVGFPS")
#lines(comparison_q1$Predicted, col = "red")
#legend(x = 100, y = 35, legend = c("Actual", "Predicted"), col = c("blue", "red"), lty = 1)
# Step 10: Plot residuals
#residuals_q1 <- comparison_q1$Actual - comparison_q1$Predicted
#plot(residuals_q1, type = "p", col = "green",
#     xlab = "Observation", ylab = "Residuals", main = "Residuals Plot")
#abline(h = 0, col = "red")
#grid()
# the % in the nodes represent the percentage of observations (n=455)
# Q2: Which of the technical variables (AA/AG/AP) have the most influence on PE?
# Step 0: Install necessary packages
# install.packages("caret")
# install.packages("rpart.plot")
# Step 1: Load and preprocess the data set
data_q2 <- read.csv("dataset_for_q2.csv")
# Step 2: Define feature and target variables
features_q2 <- c("AA", "AG", "AP")
target_q2 <- "PE"
# Step 3: Build decision tree model
library(rpart)
model_q2 <- rpart(PE ~ ., data = data_q2[, c(features_q2, target_q2)], method = "anova")
# Step 4: Split data set into training and testing sets
library(caret)
set.seed(321)
trainIndex_q2 <- createDataPartition(data_q2$PE, p = 0.7, list = FALSE)
training_q2 <- data_q2[trainIndex_q2, ]
testing_q2 <- data_q2[-trainIndex_q2, ]
# Step 5: Train the decision tree model
model_q2 <- rpart(PE ~ ., data = training_q2[, c(features_q2, target_q2)], method = "anova")
# Step 6: Model assessment
# Calculate predictions on the testing data
predictions_q2 <- predict(model_q2, testing_q2[, features_q2])
# Calculate evaluation metrics (e.g., RMSE)
accuracy_q2 <- sqrt(mean((predictions_q2 - testing_q2$PE)^2))
print("Evaluation of Model Performance")
print("RMSE (Accuracy):")
print(accuracy_q2)
# Additional evaluation metrics (e.g., R-squared)
rsquared_q2 <- 1 - (sum((testing_q2$PE - predictions_q2)^2) /
sum((testing_q2$PE - mean(testing_q2$PE))^2))
print("R-squared:")
print(rsquared_q2)
# Cross-validation (optional)
cv_q2 <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation
model_cv_q2 <- train(PE ~ ., data = data_q2[, c(features_q2, target_q2)],
method = "rpart", trControl = cv_q2)
print("Cross-validated RMSE:")
print(sqrt(model_cv_q2$results$RMSE))
# Step 7: Plot the decision tree
library(rpart.plot)
par(mfrow = c(1,1))
rpart.plot(model_q2, digits = 3, roundint = FALSE, fallen.leaves = TRUE,
main = "Decision Tree for PE Prediction")
# Step 8: Interpretation (optional)
print("Interpretation")
print("Decision Tree Model:")
print(model_q2)
print("Variable Importance Analysis:")
print(varImp(model_q2))
# Extract actual values from the testing data set
actual_q2 <- testing_q2$PE
# Create a data frame with both predicted and actual values
comparison_q2 <- data.frame(Predicted = predictions_q2, Actual = actual_q2)
# Step 9: Plot predicted vs. actual values
#par(mfrow = c(1,2))
#plot(comparison_q2$Actual, type = "l", col = "blue",
#     ylim = range(c(comparison_q2$Predicted, comparison_q2$Actual)),
#     xlab = "Observation", ylab = "PE", main = "Predicted vs. Actual PE")
#lines(comparison_q2$Predicted, col = "red")
#legend(x = 25, y =  2.2, legend = c("Actual", "Predicted"), col = c("blue", "red"), lty = 1)
# Step 10: Plot residuals
#residuals_q2 <- comparison_q2$Actual - comparison_q2$Predicted
#plot(residuals_q2, type = "p", col = "green",
#     xlab = "Observation", ylab = "Residuals", main = "Residuals Plot")
#abline(h = 0, col = "red")
#grid()
