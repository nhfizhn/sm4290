# loads the data set into R
data <- read.csv("pilot dataset - dataset.csv")
# View summary of the model
summary(model)
# View the structure of the dataset
str(data)
# Check for missing values
summary(data)
# View summary of the model
summary(model)
# Fit a regression model
model <- lm(Average_FPS ~ Audio + Graphics + Physics, data = data)
# Fit a regression model
model <- lm(AVGFPS ~ AA + AG + AP, data = data)
# View summary of the model
summary(model)
plot(data$AA, data$AVGFPS)
plot(data$AG, data$AVGFPS)
plot(data$AP, data$AVGFPS)
# checking LINE assumptions
par(mfrow = c(1,3))
plot(data$AA, data$AVGFPS)
plot(data$AG, data$AVGFPS)
plot(data$AP, data$AVGFPS)
plot(model$fitted.values, residuals(model))
hist(residuals(model))
qqnorm(residuals(model))
qqline(residuals(model))
hist(residuals(model))
qqnorm(residuals(model))
qqline(residuals(model))
shapiro.test(residuals(model))
plot(data$AVGFPS, data$AA)
# Load the rpart package
library(rpart)
# Load the rpart package
library(rpart)
# loads the data set into R
data <- read.csv("pilot dataset - dataset.csv")
# loads the data set into R
data <- read.csv("pilot dataset - dataset.csv")
# Build decision tree model
tree_model <- rpart(AVGFPS ~ AA + AG + AP, data = data)
# Plot decision tree
plot(tree_model)
text(tree_model, use.n = TRUE, cex = 0.6)
text(tree_model, use.n = TRUE, cex = 0.5)
text(tree_model, use.n = TRUE, cex = 0.4)
text(tree_model, use.n = TRUE, cex = 0.45)
# Plot decision tree
plot(tree_model)
text(tree_model, use.n = TRUE, cex = 0.45)
# Plot decision tree
plot(tree_model)
text(tree_model, use.n = TRUE, cex = 0.3)
text(tree_model, use.n = TRUE, cex = 1.5)
# Plot decision tree
plot(tree_model)
text(tree_model, use.n = TRUE, cex = 1.5)
# Plot decision tree
plot(tree_model, cex = 1.5)
# Plot decision tree
plot(tree_model, cex = 1.5, margin = 0.1)
text(tree_model, use.n = TRUE, cex = 1.5)
text(tree_model, use.n = TRUE, cex = 0.6)
# Plot decision tree
plot(tree_model, cex = 1.5, margin = 0.1)
text(tree_model, use.n = TRUE, cex = 0.6)
text(tree_model, use.n = TRUE, cex = 1.0)
(tree_model, cex = 1.5, margin = 0.1)
(tree_model, cex = 1.5, margin = 0.1)
# Plot decision tree
plot(tree_model, cex = 1.5, margin = 0.1)
text(tree_model, use.n = TRUE, cex = 1.0)
# Plot decision tree
plot(tree_model, margin = 0.1)
text(tree_model, use.n = TRUE, cex = 1.0)
# Plot decision tree
plot(tree_model, margin = 0.1)
text(tree_model, use.n = TRUE, cex = 1.0)
# Plot decision tree
plot(tree_model, margin = 0.2)
text(tree_model, use.n = TRUE, cex = 1.0)
# Plot decision tree
plot(tree_model, margin = 0.2)
text(tree_model, use.n = TRUE, cex = 0.8)
install.packages("caret")
library(caret)
# Set the number of folds (e.g., 5-fold or 10-fold)
num_folds <- 5
# Define the train control with k-fold cross-validation
train_control <- trainControl(method = "cv", number = num_folds)
data <- read.csv("pilot dataset - dataset.csv")
# Define your modeling technique and parameters (e.g., decision tree)
model <- train(AVGFPS ~ ., data = data, method = "rpart", trControl = train_control)
# View the cross-validation results
print(model)
randomForest
install.packages("randomForest")
data <- read.csv("pilot dataset - dataset.csv")
(randomForest)
(randomForest)
install.packages("randomForest")
# Load the required libraries
library(randomForest)
data <- read.csv("pilot dataset - dataset.csv")
# Create a random forest model
rf_model <- randomForest(AVGFPS ~ AA + AG + AP, data = data)
# Extract feature importance scores from the random forest model
feature_importance <- importance(rf_model)
# Print feature importance scores
print(feature_importance)
# Plot feature importance scores
varImpPlot(rf_model)
# Install and load required packages
install.packages("mco")
library(mco)
# Example dataset (replace this with your actual dataset)
# Assuming your dataset is stored in a dataframe named 'data'
data <- data.frame(
Levels = sample(1:5, 100, replace = TRUE),
Actual_Audio = sample(1:3, 100, replace = TRUE),
Actual_Graphics = sample(1:3, 100, replace = TRUE),
Actual_Physics = sample(1:3, 100, replace = TRUE),
Audio_Rating = sample(1:5, 100, replace = TRUE),
Graphics_Rating = sample(1:5, 100, replace = TRUE),
Physics_Rating = sample(1:5, 100, replace = TRUE),
Enjoyment_Rating = sample(1:5, 100, replace = TRUE),
Comfort_Rating = sample(1:5, 100, replace = TRUE),
Overall_Satisfaction_Rating = sample(1:5, 100, replace = TRUE),
Score = sample(0:1000, 100, replace = TRUE),
Average_FPS = sample(30:60, 100, replace = TRUE),
Gaming_Interest_Experience_Rating = sample(1:5, 100, replace = TRUE),
Participant_Number = sample(1:5, 100, replace = TRUE)
)
# Define objective functions
objective_functions <- function(x) {
# Objective 1: Minimize negative average FPS (convert to maximization problem)
objective_1 <- -mean(x[, "Average_FPS"])
# Objective 2: Minimize negative combined player experience rating (convert to maximization problem)
player_experience <- mean(x[, c("Enjoyment_Rating", "Comfort_Rating", "Overall_Satisfaction_Rating")])
objective_2 <- -player_experience
return(c(objective_1, objective_2))
}
# Define constraints (if any)
# For example, you may set constraints on the levels of technical resources
# Define optimization problem
problem <- mco(x0 = matrix(c(1, 1, 1), nrow = 1), fun = objective_functions)
# Install and load required packages
install.packages("mco")
library(mco)
# Example dataset (replace this with your actual dataset)
# Assuming your dataset is stored in a dataframe named 'data'
data <- data.frame(
Levels = sample(1:5, 100, replace = TRUE),
Actual_Audio = sample(1:3, 100, replace = TRUE),
Actual_Graphics = sample(1:3, 100, replace = TRUE),
Actual_Physics = sample(1:3, 100, replace = TRUE),
Audio_Rating = sample(1:5, 100, replace = TRUE),
Graphics_Rating = sample(1:5, 100, replace = TRUE),
Physics_Rating = sample(1:5, 100, replace = TRUE),
Enjoyment_Rating = sample(1:5, 100, replace = TRUE),
Comfort_Rating = sample(1:5, 100, replace = TRUE),
Overall_Satisfaction_Rating = sample(1:5, 100, replace = TRUE),
Score = sample(0:1000, 100, replace = TRUE),
Average_FPS = sample(30:60, 100, replace = TRUE),
Gaming_Interest_Experience_Rating = sample(1:5, 100, replace = TRUE),
Participant_Number = sample(1:5, 100, replace = TRUE)
)
# Define objective functions
objective_functions <- function(x) {
# Objective 1: Minimize negative average FPS (convert to maximization problem)
objective_1 <- -mean(x[, "Average_FPS"])
# Objective 2: Minimize negative combined player experience rating (convert to maximization problem)
player_experience <- mean(x[, c("Enjoyment_Rating", "Comfort_Rating", "Overall_Satisfaction_Rating")])
objective_2 <- -player_experience
return(c(objective_1, objective_2))
}
# Define optimization problem
problem <- mco(x0 = matrix(c(1, 1, 1), nrow = 1), fun = objective_functions)
# Load the required libraries
library(randomForest)
data <- read.csv("pilot dataset - dataset.csv")
# Create a random forest model
rf_model <- randomForest(AVGFPS ~ AA + AG + AP, data = pilot_training_data)
# Load the required libraries
library(randomForest)
pilot_training_data <- read.csv("pilot dataset - dataset.csv")
# Create a random forest model
rf_model <- randomForest(AVGFPS ~ AA + AG + AP, data = pilot_training_data)
# Load the required libraries
library(randomForest)
training_data <- read.csv("pilot dataset - dataset.csv")
training_data <- read.csv("pilot dataset - dataset.csv")
# Create a random forest model
rf_model <- randomForest(AVGFPS ~ AA + AG + AP, data = training_data)
predictions <- predict(rf_model, newdata = training_data)
var_importance <- importance(rf_model)
print(var_importance)
# Example of predicting average FPS and player experience for a combination of technical variables
combination <- data.frame(Audio = 2, Graphics = 3, Physics = 1)  # Example combination
predicted_fps <- predict(rf_model, newdata = combination)
# Example of predicting average FPS and player experience for a combination of technical variables
combination <- data.frame(AA = 2, AG = 3, AP = 1)  # Example combination
predicted_fps <- predict(rf_model, newdata = combination)
# Example of predicting average FPS and player experience for a combination of technical variables
combination <- data.frame(AA = 1, AG = 3, AP = 1)  # Example combination
predicted_fps <- predict(rf_model, newdata = combination)
# Load the rpart package
library(rpart)
# loads the data set into R
data <- read.csv("pilot dataset - dataset (1).csv")
# Build decision tree model
tree_model <- rpart(AVGFPS ~ AA + AG + AP, data = data)
# Plot decision tree
plot(tree_model, margin = 0.2)
text(tree_model, use.n = TRUE, cex = 0.8)
# Build decision tree model
tree_model <- rpart(NAVGFPS ~ NAA + NAG + NAP, data = data)
# Plot decision tree
plot(tree_model, margin = 0.2)
text(tree_model, use.n = TRUE, cex = 0.8)
# loads the data set into R
data <- read.csv("pilot dataset - dataset.csv")
# Build decision tree model
tree_model <- rpart(AVGFPS ~ AA + AG + AP, data = data)
# Plot decision tree
plot(tree_model, margin = 0.2)
text(tree_model, use.n = TRUE, cex = 0.8)
# Load the required libraries
library(randomForest)
data <- read.csv("pilot dataset - dataset (1).csv")
# Create a random forest model
rf_model <- randomForest(NAVGFPS ~ NAA + NAG + NAP, data = data)
# Extract feature importance scores from the random forest model
feature_importance <- importance(rf_model)
# Print feature importance scores
print(feature_importance)
# Plot feature importance scores
varImpPlot(rf_model)
# Create a random forest model
rf_avgfps_model <- randomForest(NAVGFPS ~ NAA + NAG + NAP, data = data)
rf_pe_model <- randomForest(NPE ~ NAA + NAG + NAP, data = data)
rf_avgpe_model <- randomForest(NAVGFPSPE ~ NAA + NAG + NAP, data = data)
feature_importance_1
feature_importance_1
rf_avgpe_model <- randomForest(NFPSPE ~ NAA + NAG + NAP, data = data)
# Extract feature importance scores from the random forest model
feature_importance_1 <- importance(rf_avgfps_model)
feature_importance_2 <- importance(rf_pe_model)
feature_importance_3 <- importance(rf_avgpe_model)
# Print feature importance scores
print(feature_importance_1)
print(feature_importance_2)
print(feature_importance_3)
rf_fpspe_model <- randomForest(NFPSPE ~ NAA + NAG + NAP, data = data)
feature_importance_3 <- importance(rf_fpspe_model)
# Print feature importance scores
print(feature_importance_1)
print(feature_importance_2)
print(feature_importance_3)
# Plot feature importance scores
par(mfrow <- c(1,3))
# Plot feature importance scores
par(mfrow = c(1,3))
varImpPlot(rf_avgfps_model)
varImpPlot(rf_avgfps_model)
varImpPlot(rf_avgfps_model)
varImpPlot(rf_avgfps_model)
varImpPlot(rf_pe_model)
varImpPlot(rf_fpspe_model)
gc()
# Load the required libraries
library(randomForest)
data <- read.csv("pilot dataset - dataset (1).csv")
# Create a random forest model
rf_avgfps_model <- randomForest(NAVGFPS ~ NAA + NAG + NAP, data = data)
rf_pe_model <- randomForest(NPE ~ NAA + NAG + NAP, data = data)
rf_fpspe_model <- randomForest(NFPSPE ~ NAA + NAG + NAP, data = data)
# Extract feature importance scores from the random forest model
feature_importance_1 <- importance(rf_avgfps_model)
feature_importance_2 <- importance(rf_pe_model)
feature_importance_3 <- importance(rf_fpspe_model)
# Print feature importance scores
print(feature_importance_1)
print(feature_importance_2)
print(feature_importance_3)
# Plot feature importance scores
par(mfrow = c(1,3))
varImpPlot(rf_avgfps_model)
varImpPlot(rf_pe_model)
varImpPlot(rf_fpspe_model)
View(rf_avgfps_model)
#-------------------------------------------------------------------------------
library(rpart.plot)
install.packages("rpart.plot")
#-------------------------------------------------------------------------------
library(rpart.plot)
rf_model <- randomForest(NFPSPE ~ NAA + NAG + NAP, data = data)
# Plotting the first tree
rpart.plot(rf_model$forest[[1]], extra = 102, type = 2)
#-------------------------------------------------------------------------------
library(rparT)
#-------------------------------------------------------------------------------
library(rpart)
# Plotting the first tree
plot(rf_model$forest[[1]], main = "Random Forest Tree")
# Feature importance
importance(rf_model)
# Example prediction
new_data <- data.frame(NAA = 2, NAG = 3, NAP = 1)
predict(rf_model, new_data)
# Example prediction
new_data <- data.frame(NAA = 0.5, NAG = 1, NAP = 0)
predict(rf_model, new_data)
# Load the dataset
data <- read.csv("pilot dataset - dataset (1).csv")
# Summary statistics
summary(data)
# Histograms
par(mfrow=c(2, 3)) # Set up a 2x3 grid of plots
hist(data$AA, main="Actual Audio")
hist(data$AG, main="Actual Graphics")
hist(data$AP, main="Actual Physics")
hist(data$AVGFPS, main="AVGFPS")
hist(data$PE, main="PE")
par(mfrow=c(1, 1)) # Reset to default plot layout
# Scatter plots
plot(data$AA, data$AVGFPS, xlab="Actual Audio", ylab="AVGFPS", main="Scatter Plot")
plot(data$AG, data$AVGFPS, xlab="Actual Graphics", ylab="AVGFPS", main="Scatter Plot")
# Histograms
par(mfrow=c(2, 3)) # Set up a 2x3 grid of plots
# Scatter plots
plot(data$AA, data$AVGFPS, xlab="Actual Audio", ylab="AVGFPS", main="Scatter Plot")
plot(data$AG, data$AVGFPS, xlab="Actual Graphics", ylab="AVGFPS", main="Scatter Plot")
plot(data$AP, data$AVGFPS, xlab="Actual Physics", ylab="AVGFPS", main="Scatter Plot")
plot(data$AA, data$PE, xlab="Actual Audio", ylab="PE", main="Scatter Plot")
plot(data$AG, data$PE, xlab="Actual Graphics", ylab="PE", main="Scatter Plot")
plot(data$AP, data$PE, xlab="Actual Physics", ylab="PE", main="Scatter Plot")
# Correlation matrix
par(mfrow=c(1, 1)) # Reset to default plot layout
correlation_matrix <- cor(data[, c("AA", "AG", "AP", "AVGFPS", "PE")])
print(correlation_matrix)
heatmap(correlation_matrix, symm=TRUE)
# Boxplots for outliers
par(mfrow=c(1, 2)) # Set up a 1x2 grid of plots
boxplot(data$AVGFPS, main="AVGFPS Boxplot")
boxplot(data$PE, main="PE Boxplot")
# Check for missing values
missing_values <- sum(is.na(data))
print(paste("Number of missing values:", missing_values))
# Histograms
par(mfrow=c(2, 3)) # Set up a 2x3 grid of plots
# Jittered scatter plots
plot(jitter(data$AA), data$AVGFPS, xlab="AA", ylab="AVGFPS", main="Scatter Plot with Jitter: AA vs AVGFPS")
plot(jitter(data$AG), data$AVGFPS, xlab="AG", ylab="AVGFPS", main="Scatter Plot with Jitter: AG vs AVGFPS")
plot(jitter(data$AP), data$AVGFPS, xlab="AP", ylab="AVGFPS", main="Scatter Plot with Jitter: AP vs AVGFPS")
plot(jitter(data$AA), data$PE, xlab="AA", ylab="PE", main="Scatter Plot with Jitter: AA vs PE")
plot(jitter(data$AG), data$PE, xlab="AG", ylab="PE", main="Scatter Plot with Jitter: AG vs PE")
plot(jitter(data$AP), data$PE, xlab="AP", ylab="PE", main="Scatter Plot with Jitter: AP vs PE")
# Calculate average AVGFPS and PE for each level
avg_AVGFPS <- tapply(data$AVGFPS, data$Levels, mean)
# Create bar plots
barplot(AVGFPS, main="Average AVGFPS by Level", xlab="Level", ylab="Average AVGFPS", col="skyblue")
# Calculate average AVGFPS and PE for each level
avg_AVGFPS <- tapply(data$AVGFPS, data$Levels, mean)
avg_PE <- tapply(data$PE, data$Levels, mean)
# Create bar plots
barplot(avg_AVGFPS, main="Average AVGFPS by Level", xlab="Level", ylab="Average AVGFPS", col="skyblue")
barplot(avg_PE, main="Average PE by Level", xlab="Level", ylab="Average PE", col="lightgreen")
# Load the dataset
data <- read.csv("pilot dataset - dataset (1).csv")
# Check the structure of the dataset
str(data)
# Fit a multiple linear regression model
# Assuming AVGFPS is the dependent variable and AA, AG, and AP are the independent variables
model <- lm(AVGFPS ~ AA + AG + AP, data = data)
# Summary of the model
summary(model)
# Scatterplot of observed vs predicted AVGFPS
plot(data$AVGFPS, fitted(model), xlab = "Observed AVGFPS", ylab = "Predicted AVGFPS")
# Scatterplot of observed vs predicted AVGFPS
par(mfrow = c(1,1)
plot(data$AVGFPS, fitted(model), xlab = "Observed AVGFPS", ylab = "Predicted AVGFPS")
# Scatterplot of observed vs predicted AVGFPS
par(mfrow = c(1,1))
plot(data$AVGFPS, fitted(model), xlab = "Observed AVGFPS", ylab = "Predicted AVGFPS")
model
abline(h = 0, col = "red")  # Add a horizontal line at y = 0 for reference
# Scatterplot of observed vs predicted AVGFPS
par(mfrow = c(1,2))
plot(data$AVGFPS, fitted(model), xlab = "Observed AVGFPS", ylab = "Predicted AVGFPS")
# Residual plot
plot(residuals(model) ~ fitted(model), xlab = "Fitted Values", ylab = "Residuals")
abline(h = 0, col = "red")  # Add a horizontal line at y = 0 for reference
# Summary of the model
summary(model)
# Fit a multiple linear regression model
# Assuming AVGFPS is the dependent variable and AA, AG, and AP are the independent variables
model <- lm(NAVGFPS ~ NAA + NAG + NAP, data = data)
# Summary of the model
summary(model)
# Scatterplot of observed vs predicted AVGFPS
par(mfrow = c(1,2))
plot(data$AVGFPS, fitted(model), xlab = "Observed AVGFPS", ylab = "Predicted AVGFPS")
# Residual plot
plot(residuals(model) ~ fitted(model), xlab = "Fitted Values", ylab = "Residuals")
abline(h = 0, col = "red")  # Add a horizontal line at y = 0 for reference
